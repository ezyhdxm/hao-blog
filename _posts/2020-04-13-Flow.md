---
layout: post
title:  "Flow"
comments: true
date:   2020-04-13 19:35:00 +0800
tags: generative
lang: zh
---

> 


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

## 一维流：基本想法

由于我们的数据现在是连续的，因此我们想拟合的就是一个概率密度模型。为了能表示尽可能多的概率模型，一个直接的想法就是将样本丢入一个神经网络中，让神经网络给出一个概率密度函数。但概率密度函数是受到一定约束的，比如说$$\int_{-\infty}^{+\infty} p_{\theta}(x) d x=1$$，我们很难直接给神经网络加上这样的要求。若我们想着事后再做归一化，训练的时候只是要求输出的函数不小于零，那么由于我们的目标函数是$$\max _{\theta} \sum_{i} \log p_{\theta}\left(x^{(i)}\right)$$，神经网络会指引着每个$$p_{\theta}\left(x^{(i)}\right)$$朝着正无穷前进，使我们最后得不到有意义的结果。那我们要怎么办呢？

流模型（Flow Model）另辟蹊径，大约是受到随机变量替换的启发，它决定用神经网络将输入的随机变量$$x$$变换为另一个分布简单的随机变量$$z$$，即$$z=f_{\theta}(x)$$，$$z \sim p_{Z}(z)$$。所谓流就是一个严格单调连续的函数。由于有可逆分布函数的随机变量可以由流互相转换，故流模型能够估计的概率密度函数是较为一般的。

### 训练及抽样

首先我们要将目标函数写出来。在一维情形，某一点$$x$$附近的概率可以由 $$p_{\theta}(x) d x$$，在进行可逆变换$$z=f_{\theta}(x)$$后，对应点的概率应该是不变的，即有$$p_{\theta}(x) d x=p_{Z}(z) d z$$。我们要求变换同时是可微的，那么就有

$$\begin{equation}p_{\theta}(x)=p\left(f_{\theta}(x)\right)\left|\frac{\partial f_{\theta}(x)}{\partial x}\right| \tag{1} \label{eq:1}\end{equation}$$

这也就是一维情形下的变量替换公式。将它代入目标函数之中，得到

$$\begin{equation}\max _{\theta} \sum_{i} \log p_{\theta}\left(x^{(i)}\right)=\max _{\theta} \sum_{i} \log p_{Z}\left(f_{\theta}\left(x^{(i)}\right)\right)+\log \left|\frac{\partial f_{\theta}}{\partial x}\left(x^{(i)}\right)\right|\tag{2} \label{eq:2}\end{equation}$$

这以后我们就可以用自己喜爱的梯度下降方法来进行训练了。

## 从二维到$$n$$维


